[project]
name = "gary"
version = "0.1.0"
description = "A testing implementation of the LLM side of the Neuro-sama Game SDK, based around the Guidance project."
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "guidance>=0.2.1",
    "llama-cpp-python>=0.3.7", # undeclared guidance dependency
    "fastapi[standard]>=0.115.7",
    "pydantic>=2.11.3",
    "websockets>=14.2",
    "colorlog>=6.9.0",
    "panel==1.6.1", # TODO: 1.6.2 has new inconsistent toggleswitch css + actions list no longer limits its height
    "jsonschema>=4.23.0",
    "jsf>=0.11.2",
    "orjson>=3.10.15",
    "loguru>=0.7.3",
]

[project.optional-dependencies]
transformers = [
    "transformers>=4.52.4",
    "torch>=2.6.0",
    "accelerate>=1.7.0",
]

[tool.uv]
package = true

[project.scripts]
gary = "gary.__main__:start"
schema_test = "tests.schema_test:start"
check_keywords = "tests.check_keywords:test_keywords"

[tool.uv.sources]
# llama-cpp-python = { index = "abetlen-cu124" } # not updated past 0.3.4
llama-cpp-python = { url = "https://github.com/JamePeng/llama-cpp-python/releases/download/v0.3.8-cu124-AVX2-win-20250519/llama_cpp_python-0.3.8-cp312-cp312-win_amd64.whl" }
torch = { index = "pytorch-cu124" }

[[tool.uv.index]]
name = "pytorch-cu124"
url = "https://download.pytorch.org/whl/cu124"
explicit = true # they host jinja2 too for some reason

[[tool.uv.index]]
name = "abetlen-cu124"
url = "https://abetlen.github.io/llama-cpp-python/whl/cu124/"

[tool.ruff.lint]
ignore = [
    "E401", # multiple imports on one line
    "F403", # star imports
    "F405"  # symbol from star imports
]

[dependency-groups]
dev = [
    "ipykernel>=6.29.5",
    "jupyter-bokeh>=4.0.5",
    "gpustat>=1.1.1",
    "guidance-stitch>=0.1.0",
]
