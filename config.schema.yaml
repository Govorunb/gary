# yaml-language-server: $schema=https://json-schema.org/draft-07/schema
title: Gary Config Schema
$id: gary-config
additionalProperties:
  oneOf:
    - {$ref: "#/definitions/preset_base"}
    - {$ref: "#/definitions/preset_inherit"}

definitions:
  preset_base:
    description: Preset
    type: object
    properties:
      fastapi:
        type: object
        additionalProperties: true
        properties:
          host: {type: string}
          port: {type: integer, minimum: 1, maximum: 65535}
          reload: {type: boolean}
      llm:
        type: object
        properties:
          engine: {$ref: "#/definitions/engine"}
          model:
            type: string
            description: |
              For local engines (e.g. llama_cpp), use the local path to the model (e.g. `_models/llama-2-7b.gguf`).
              For remote services like Anthropic, use the name of the model.
              For guidance_server, this should be the URL of the server.
          api_key: {type: string}
        required: [engine, model]
      engine_params:
        type: object
        description: |
          Parameters to pass to your chosen engine. Refer to the engine's documentation for possible values.
        additionalProperties: true
      gary:
        type: object
        properties:
          log_level_file: {$ref: "#/definitions/log_level"}
          log_level_console: {$ref: "#/definitions/log_level"}
          enable_cot:
            type: boolean
            description: |
              Enable poor man's chain-of-thought.
              In my testing, this just eats through tokens without a tangible difference.
              A lot of the time, the 'reasoning' is full of hallucinations and the model chooses to do something irrelevant anyway.
          non_ephemeral_try_context:
            type: boolean
            description: |
              Don't discard context from picking whether to act or not.
              If the model happens to generate good reasoning for it, this will help when picking the action.
              If your model hallucinates a lot, it will very much not help.
              You should play around with this.
          act_on_context:
            type: boolean
            description: |
              Try to act on receiving (non-silent) context.
              Can make decisions marginally smarter since the most relevant information is closest to the end of the context.
          try_on_register:
            type: boolean
            description: |
              After receiving `actions/register`, try to act immediately.
              Can be hit or miss.
          existing_connection_policy:
            enum: [disconnect_new, disconnect_existing]
            description: |
              What to do when someone tries to connect to a game that already has an active connection.
          scheduler:
            type: object
            properties: 
              idle_timeout_try:
                type: number
                minimum: 0
                description: |
                  If the LLM does not act for this many seconds, manually ask it to act (it may still decide not to).
                  0 to disable.
              idle_timeout_force:
                type: number
                minimum: 0
                description: |
                  If the LLM does not act for this many seconds, force it to pick an action to perform.
                  0 to disable.
    additionalProperties: false
  preset_inherit:
    description: Inherit from another preset
    type: object
    properties:
      base: {type: string}
      overrides: {$ref: "#/definitions/preset_base"}
    required: [base, overrides]
  engine: {enum: [openai, anthropic, azure_openai, googleai, llama_cpp, transformers, guidance_server]}
  log_level:
    description: Minimum log level. This can be one of the given values, or a non-negative integer (refer to python's logging module).
    oneOf:
      - enum: [all, debug, info, warn, warning, error, critical, fatal, none]
      - type: integer
        minimum: 0
