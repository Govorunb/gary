# yaml-language-server: $schema=config.schema.yaml
default:
  fastapi:
    host: 127.0.0.1
    port: 8000
    reload: false
  llm:
    engine: llama_cpp
    model: _models/bartowski_Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf
  engine_params:
    n_gpu_layers: -1
    n_ctx: 8192
    temperature: 1.6
    flash_attn: true
  gary:
    log_level_file: info
    log_level_console: debug
    enable_cot: false
    non_ephemeral_try_context: false
    scheduler:
      idle_timeout_try: 6
      idle_timeout_force: 30
branching-paths:
  base: default
  overrides:
    gary:
      log_level_console: info
      scheduler:
        idle_timeout_try: 3
        idle_timeout_force: 10
phi:
  base: default
  overrides:
    llm:
      engine: llama_cpp
      model: _models/bartowski_Phi-3.5-mini-instruct-Q4_K_L.gguf
    engine_params:
      n_ctx: 4096
      temperature: 1.0
    gary:
      enable_cot: true
      non_ephemeral_try_context: true
openai:
  base: default
  overrides:
    llm:
      engine: openai
      model: gpt-4o-mini
      api_key: $ENV:OPENAI_API_KEY
    engine_params:
      temperature: 0.5
