# yaml-language-server: $schema=config.schema.yaml
default:
  api:
    host: 127.0.0.1
    port: 8000
    reload: false
  llm:
    engine: llama_cpp
    model: _models/bartowski_Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf
  engine_params:
    n_gpu_layers: -1
    n_ctx: 8192
    temperature: 0.3 # seems to not do much
    flash_attn: true # speeds up inference
  gary:
    log_level_file: info
    log_level_console: info
    enable_cot: false
    non_ephemeral_try_context: false
    scheduler:
      idle_timeout_try: 6
      idle_timeout_force: 30
randy:
  base: default
  overrides:
    llm:
      engine: randy
      model: ""
    gary:
      enable_cot: false # you don't want randy with this on, trust me
      non_ephemeral_try_context: false
      scheduler:
        idle_timeout_try: 0
        idle_timeout_force: 5
phi:
  base: default
  overrides:
    llm:
      engine: llama_cpp
      model: _models/bartowski_Phi-3.5-mini-instruct-Q4_K_L.gguf
    gary:
      enable_cot: true
      non_ephemeral_try_context: true
