# yaml-language-server: $schema=config.schema.yaml
default:
  api:
    host: 127.0.0.1
    port: 8000
    reload: false
  llm:
    engine: llama_cpp
    model: _models/bartowski_Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf
  engine_params:
    n_gpu_layers: -1
    n_ctx: 8192
    temperature: 0.3 # seems to not do much
    flash_attn: true # speeds up inference
  gary:
    allow_yapping: false
    scheduler:
      idle_timeout_try: 6
      idle_timeout_force: 30
    logging:
      log_level_file: info
      log_level_console: info
      modules:
        scheduler: warning
        llm: info
randy:
  base: default
  overrides:
    llm:
      engine: randy
    gary:
      allow_yapping: false # you don't want randy with this on, trust me
      scheduler:
        idle_timeout_try: 0
        idle_timeout_force: 5
phi:
  base: default
  overrides:
    llm:
      engine: llama_cpp
      model: _models/bartowski_Phi-3.5-mini-instruct-Q4_K_L.gguf
    gary:
      allow_yapping: true
      scheduler:
        sleep_after_say: true
debug_websocket:
  base: randy
  overrides:
    gary:
      logging:
        log_level_console: trace
        modules:
          websocket: trace # logs raw websocket messages
