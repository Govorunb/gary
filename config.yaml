# yaml-language-server: $schema=config.schema.yaml
default:
  fastapi:
    host: 127.0.0.1
    port: 8000
    reload: true
  llm:
    engine: llama_cpp
    model: _models/bartowski_Meta-Llama-3.1-8B-Instruct-Q5_K_L.gguf
  engine_params:
    n_gpu_layers: -1
    n_ctx: 4096
    temperature: 0.6
  gary:
    log_level_file: info
    log_level_console: debug
    enable_cot: false
    non_ephemeral_try_context: false
    try_on_register: false
phi:
  base: default
  overrides:
    llm:
      engine: llama_cpp
      model: _models/bartowski_Phi-3.1-mini-128k-instruct-Q6_K_L.gguf
    engine_params:
      n_gpu_layers: -1
      n_ctx: 8192
      temperature: 1.0
openai:
  base: default
  overrides:
    llm:
      engine: openai
      model: gpt-4o-mini
      api_key: $OPENAI_API_KEY
    engine_params:
      temperature: 0.5
